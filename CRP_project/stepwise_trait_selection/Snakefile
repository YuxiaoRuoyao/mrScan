import pandas as pd
import random
import string
import os.path
import glob
import shutil
from snakemake.utils import validate
from snakemake.io import glob_wildcards

configfile: "config.yaml"
data_dir = config["out"]["data_dir"] #where the data is
result_dir = config["out"]["result_dir"] #where results will go
out_dir = config["out"]["output_dir"] #where the previous results
previous_data_dir = config["out"]["previous_data_dir"] #where the previous data
original_gwas_dir = config["out"]["gwas_dir"] # where originally download gwas data lives

prefix = config["input"]["label"] + "_"
id_exposure = config["input"]["id_exposure"]
id_outcome = config["input"]["id_outcome"]
type_outcome = config["input"]["type_outcome"]
prevalence_outcome = config["input"]["prevalence_outcome"]
l2_dir = config["analysis"]["estimate_R"]["l2_dir"] # where LD score reference data lives

rule all:
    input:
        expand(out_dir + prefix + "final_{downstream_method}_FDR_p_{downstream_p}_{file_type}.csv", 
               downstream_method = config["analysis"]["downstream_filter"]["method"],
               downstream_p = config["analysis"]["downstream_filter"]["p"],
               file_type = ["id_list","trait_info"])
    
rule combine_gwas:
    input: id_list = out_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_id_list.csv",
           trait_info = out_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_trait_info.csv",
           file_info_exposure_outcome = out_dir + "df_info_exposure_outcome.csv",
           download = previous_data_dir + prefix + "download_{downstream_method}_FDR_p_{downstream_p}.sh"
    params: path = original_gwas_dir,
            id_exposure = id_exposure,
            id_outcome = id_outcome
    output: out = previous_data_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_beta.{chrom}.RDS"
    wildcard_constraints: chrom = "\d+",
                          downstream_method = ".*(MR|MVMR).*",
                          downstream_p = "[\d.]+"
    script: "/nfs/turbo/sph-jvmorr/CRP_project/pipeline/trait_selection/R/combine_gwas.R"

rule calculate_cor:
    input: beta = expand(previous_data_dir + prefix + "unique_traits_{{downstream_method}}_FDR_p_{{downstream_p}}_beta.{chrom}.RDS", chrom = range(1, 23)),
           m = expand(l2_dir + "{chrom}.l2.M_5_50", chrom = range(1, 23)),
           l2 = expand(l2_dir + "{chrom}.l2.ldscore.gz", chrom = range(1, 23))
    output: out = out_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_R_ldsc.RDS"
    wildcard_constraints: downstream_method = ".*(MR|MVMR).*",
                          downstream_p = "[\d.]+"
    script: "/nfs/turbo/sph-jvmorr/CRP_project/pipeline/trait_selection/R/ldsc_full.R"

def get_filter_data_input(wildcards):
    downstream_method = wildcards.downstream_method
    downstream_p = wildcards.downstream_p
    n_old = int(wildcards.n) - 1
    return result_dir + prefix + f"{downstream_method}_FDR_p_{downstream_p}_id_list_n{n_old}.csv"
        
rule filter_data:
    input:
        beta = previous_data_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_beta.{chrom}.RDS",
        id_list = get_filter_data_input
    params: id_exposure = id_exposure,
            id_outcome = id_outcome
    output:
        out = data_dir + prefix + "beta_filtered.{chrom}_{downstream_method}_FDR_p_{downstream_p}_n{n}.RDS"
    script:
        "/nfs/turbo/sph-jvmorr/CRP_project/pipeline/trait_selection/R/filter_data.R"

def get_ld_clumping_input(wildcards):
    n = int(wildcards.n)
    chrom = wildcards.chrom
    downstream_method = wildcards.downstream_method
    downstream_p = wildcards.downstream_p
    if n == 0:
        return previous_data_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_beta." + chrom + ".RDS"
    else:
        return data_dir + prefix + f"beta_filtered." + chrom + f"_{downstream_method}_FDR_p_{downstream_p}_n{n}.RDS"

checkpoint LD_clumping:
    input: beta = get_ld_clumping_input
    params: r2_thresh = config["analysis"]["ldprune"]["r2_thresh"],
            clump_kb = config["analysis"]["ldprune"]["clump_kb"],
            ref_path = config["analysis"]["ldprune"]["ref_path"],
            ld_prioritization = config["analysis"]["ldprune"]["ld_prioritization"],
            pthresh = config["analysis"]["ldprune"]["pthresh"]
    output: out = data_dir + prefix + "ldpruned.{chrom}_{downstream_method}_FDR_p_{downstream_p}_n{n}.RDS"
    wildcard_constraints: chrom = "\d+",
                          downstream_method = ".*(MR|MVMR).*",
                          downstream_p = "[\d.]+"
    script: "/nfs/turbo/sph-jvmorr/CRP_project/pipeline/trait_selection/R/ld_prune_plink.R"
    
def get_strength_filter_input(wildcards):
   n = int(wildcards.n)
   return expand(data_dir + prefix + "ldpruned.{chrom}_{{downstream_method}}_FDR_p_{{downstream_p}}_n{n}.RDS", chrom=range(1, 23), n=str(n))
        
checkpoint strength_filter:
    input: beta = get_strength_filter_input,
           R = out_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_R_ldsc.RDS",
           trait_info = out_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_trait_info.csv"
    params: pval_threshold = config["analysis"]["strength_filter"]["pval_threshold"],
            F_threshold = config["analysis"]["strength_filter"]["F_threshold"],
            effect_size_cutoff = config["analysis"]["strength_filter"]["effect_size_cutoff"],
            type_outcome = type_outcome,
            prevalence_outcome = prevalence_outcome,
            extra_traits =  config["analysis"]["strength_filter"]["extra_traits"],
            id_exposure = id_exposure,
            id_outcome = id_outcome
    output: out_id_list = result_dir + prefix + "{downstream_method}_FDR_p_{downstream_p}_id_list_n{n}.csv",
            out_df_strength = result_dir + prefix + "{downstream_method}_FDR_p_{downstream_p}_df_strength_n{n}.csv",
            out_flag = result_dir + prefix + "{downstream_method}_FDR_p_{downstream_p}_status_n{n}.flag"
    wildcard_constraints: downstream_method = ".*(MR|MVMR).*",
                          downstream_p = "[\d.]+"
    script: "/nfs/turbo/sph-jvmorr/CRP_project/pipeline/trait_selection/R/strength_filter.R"

def next_input(wcs):
    flag_files = sorted(glob.glob(f'{result_dir}{prefix}{wcs.downstream_method}_FDR_p_{wcs.downstream_p}_status_n*.flag'))
    if flag_files:
        n = max(int(f.split('_n')[-1].split('.')[0]) for f in flag_files)
        latest_flag_file = f"{result_dir}{prefix}{wcs.downstream_method}_FDR_p_{wcs.downstream_p}_status_n{n}.flag"
        with open(latest_flag_file, 'r') as file:
            flag_status = file.read().strip()
        if flag_status == "CONTINUE":
            n += 1
    else: 
        n = 0
    outputs = checkpoints.strength_filter.get(n=n, **wcs).output
    return outputs["out_flag"], outputs["out_id_list"]

rule finalize:
    input:
        flag_and_id_list = next_input
    output:
        final_list1 = result_dir + prefix + "final_{downstream_method}_FDR_p_{downstream_p}_id_list.csv",
        final_list2 = out_dir + prefix + "final_{downstream_method}_FDR_p_{downstream_p}_id_list.csv"
    run:
        flag_file, id_list_file = input.flag_and_id_list
        flag_status = open(flag_file).read().strip()
        if flag_status == "STOP":
            shell(f"cp {id_list_file} {output.final_list1}")
            shell(f"cp {id_list_file} {output.final_list2}")
    
rule info_file:
    input: id_list = out_dir + prefix + "final_{downstream_method}_FDR_p_{downstream_p}_id_list.csv",
           trait_info = out_dir + prefix + "unique_traits_{downstream_method}_FDR_p_{downstream_p}_trait_info.csv"
    output: out = out_dir + prefix + "final_{downstream_method}_FDR_p_{downstream_p}_trait_info.csv"
    script: "/nfs/turbo/sph-jvmorr/CRP_project/pipeline/trait_selection/R/generate_info_file.R"

